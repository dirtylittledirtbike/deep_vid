{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import IPython.display as display\n",
    "import PIL.Image\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 186\n",
      "frame_0.jpg\n",
      "frame_1.jpg\n",
      "frame_2.jpg\n",
      "frame_3.jpg\n",
      "frame_4.jpg\n",
      "frame_5.jpg\n",
      "frame_6.jpg\n",
      "frame_7.jpg\n",
      "frame_8.jpg\n",
      "frame_9.jpg\n",
      "frame_10.jpg\n",
      "frame_11.jpg\n",
      "frame_12.jpg\n",
      "frame_13.jpg\n",
      "frame_14.jpg\n",
      "frame_15.jpg\n",
      "frame_16.jpg\n",
      "frame_17.jpg\n",
      "frame_18.jpg\n",
      "frame_19.jpg\n",
      "frame_20.jpg\n",
      "frame_21.jpg\n",
      "frame_22.jpg\n",
      "frame_23.jpg\n",
      "frame_24.jpg\n",
      "frame_25.jpg\n",
      "frame_26.jpg\n",
      "frame_27.jpg\n",
      "frame_28.jpg\n",
      "frame_29.jpg\n",
      "frame_30.jpg\n",
      "frame_31.jpg\n",
      "frame_32.jpg\n",
      "frame_33.jpg\n",
      "frame_34.jpg\n",
      "frame_35.jpg\n",
      "frame_36.jpg\n",
      "frame_37.jpg\n",
      "frame_38.jpg\n",
      "frame_39.jpg\n",
      "frame_40.jpg\n",
      "frame_41.jpg\n",
      "frame_42.jpg\n",
      "frame_43.jpg\n",
      "frame_44.jpg\n",
      "frame_45.jpg\n",
      "frame_46.jpg\n",
      "frame_47.jpg\n",
      "frame_48.jpg\n",
      "frame_49.jpg\n",
      "frame_50.jpg\n",
      "frame_51.jpg\n",
      "frame_52.jpg\n",
      "frame_53.jpg\n",
      "frame_54.jpg\n",
      "frame_55.jpg\n",
      "frame_56.jpg\n",
      "frame_57.jpg\n",
      "frame_58.jpg\n",
      "frame_59.jpg\n",
      "frame_60.jpg\n",
      "frame_61.jpg\n",
      "frame_62.jpg\n",
      "frame_63.jpg\n",
      "frame_64.jpg\n",
      "frame_65.jpg\n",
      "frame_66.jpg\n",
      "frame_67.jpg\n",
      "frame_68.jpg\n",
      "frame_69.jpg\n",
      "frame_70.jpg\n",
      "frame_71.jpg\n",
      "frame_72.jpg\n",
      "frame_73.jpg\n",
      "frame_74.jpg\n",
      "frame_75.jpg\n",
      "frame_76.jpg\n",
      "frame_77.jpg\n",
      "frame_78.jpg\n",
      "frame_79.jpg\n",
      "frame_80.jpg\n",
      "frame_81.jpg\n",
      "frame_82.jpg\n",
      "frame_83.jpg\n",
      "frame_84.jpg\n",
      "frame_85.jpg\n",
      "frame_86.jpg\n",
      "frame_87.jpg\n",
      "frame_88.jpg\n",
      "frame_89.jpg\n",
      "frame_90.jpg\n",
      "frame_91.jpg\n",
      "frame_92.jpg\n",
      "frame_93.jpg\n",
      "frame_94.jpg\n",
      "frame_95.jpg\n",
      "frame_96.jpg\n",
      "frame_97.jpg\n",
      "frame_98.jpg\n",
      "frame_99.jpg\n",
      "frame_100.jpg\n",
      "frame_101.jpg\n",
      "frame_102.jpg\n",
      "frame_103.jpg\n",
      "frame_104.jpg\n",
      "frame_105.jpg\n",
      "frame_106.jpg\n",
      "frame_107.jpg\n",
      "frame_108.jpg\n",
      "frame_109.jpg\n",
      "frame_110.jpg\n",
      "frame_111.jpg\n",
      "frame_112.jpg\n",
      "frame_113.jpg\n",
      "frame_114.jpg\n",
      "frame_115.jpg\n",
      "frame_116.jpg\n",
      "frame_117.jpg\n",
      "frame_118.jpg\n",
      "frame_119.jpg\n",
      "frame_120.jpg\n",
      "frame_121.jpg\n",
      "frame_122.jpg\n",
      "frame_123.jpg\n",
      "frame_124.jpg\n",
      "frame_125.jpg\n",
      "frame_126.jpg\n",
      "frame_127.jpg\n",
      "frame_128.jpg\n",
      "frame_129.jpg\n",
      "frame_130.jpg\n",
      "frame_131.jpg\n",
      "frame_132.jpg\n",
      "frame_133.jpg\n",
      "frame_134.jpg\n",
      "frame_135.jpg\n",
      "frame_136.jpg\n",
      "frame_137.jpg\n",
      "frame_138.jpg\n",
      "frame_139.jpg\n",
      "frame_140.jpg\n",
      "frame_141.jpg\n",
      "frame_142.jpg\n",
      "frame_143.jpg\n",
      "frame_144.jpg\n",
      "frame_145.jpg\n",
      "frame_146.jpg\n",
      "frame_147.jpg\n",
      "frame_148.jpg\n",
      "frame_149.jpg\n",
      "frame_150.jpg\n",
      "frame_151.jpg\n",
      "frame_152.jpg\n",
      "frame_153.jpg\n",
      "frame_154.jpg\n",
      "frame_155.jpg\n",
      "frame_156.jpg\n",
      "frame_157.jpg\n",
      "frame_158.jpg\n",
      "frame_159.jpg\n",
      "frame_160.jpg\n",
      "frame_161.jpg\n",
      "frame_162.jpg\n",
      "frame_163.jpg\n",
      "frame_164.jpg\n",
      "frame_165.jpg\n",
      "frame_166.jpg\n",
      "frame_167.jpg\n",
      "frame_168.jpg\n",
      "frame_169.jpg\n",
      "frame_170.jpg\n",
      "frame_171.jpg\n",
      "frame_172.jpg\n",
      "frame_173.jpg\n",
      "frame_174.jpg\n",
      "frame_175.jpg\n",
      "frame_176.jpg\n",
      "frame_177.jpg\n",
      "frame_178.jpg\n",
      "frame_179.jpg\n",
      "frame_180.jpg\n",
      "frame_181.jpg\n",
      "frame_182.jpg\n",
      "frame_183.jpg\n",
      "frame_184.jpg\n",
      "frame_185.jpg\n"
     ]
    }
   ],
   "source": [
    "def img_to_numpy(img_path, max_dim=None):\n",
    "    image_path = img_path\n",
    "    img = PIL.Image.open(image_path)\n",
    "    if max_dim:\n",
    "        img.thumbnail((max_dim, max_dim))\n",
    "    return np.array(img)\n",
    "\n",
    "# Normalize an image\n",
    "def deprocess(img):\n",
    "    img = 255*(img + 1.0)/2.0\n",
    "    return tf.cast(img, tf.uint8)\n",
    "\n",
    "# Display an image\n",
    "def show(img):\n",
    "    display.display(PIL.Image.fromarray(np.array(img)))\n",
    "\n",
    "\n",
    "image_files = []\n",
    "numpy_images = []\n",
    "\n",
    "path, dirs, files = next(os.walk(\"./data/\"))\n",
    "file_count = len(files) - 1\n",
    "print(\"images:\", file_count)\n",
    "\n",
    "# iterate through files in /data directory\n",
    "for i in range(0, file_count):\n",
    "    print('frame_' + str(i) + '.jpg')\n",
    "    image_files.append(path +'frame_' + str(i) + '.jpg')\n",
    "    \n",
    "# convert those image files to numpy array\n",
    "for file in image_files:\n",
    "    numpy_images.append(img_to_numpy(file, max_dim=500))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(img, model):\n",
    "  # Pass forward the image through the model to retrieve the activations.\n",
    "  # Converts the image into a batch of size 1.\n",
    "    img_batch = tf.expand_dims(img, axis=0)\n",
    "    layer_activations = model(img_batch)\n",
    "    if len(layer_activations) == 1:\n",
    "        layer_activations = [layer_activations]\n",
    "\n",
    "    losses = []\n",
    "    for act in layer_activations:\n",
    "        loss = tf.math.reduce_mean(act)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return  tf.reduce_sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDream(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=(\n",
    "            tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=[], dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=[], dtype=tf.float32),)\n",
    "    )\n",
    "    \n",
    "    def __call__(self, img, steps, step_size):\n",
    "        print(\"Tracing\")\n",
    "        loss = tf.constant(0.0)\n",
    "        for n in tf.range(steps):\n",
    "            with tf.GradientTape() as tape:\n",
    "              # This needs gradients relative to `img`\n",
    "              # `GradientTape` only watches `tf.Variable`s by default\n",
    "                tape.watch(img)\n",
    "                loss = calc_loss(img, self.model)\n",
    "\n",
    "            # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
    "            gradients = tape.gradient(loss, img)\n",
    "\n",
    "            # Normalize the gradients.\n",
    "            gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "        \n",
    "            # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
    "            # You can update the image by directly adding the gradients (because they're the same shape!)\n",
    "            img = img + gradients*step_size\n",
    "            img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "        return loss, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_roll(img, maxroll):\n",
    "  # Randomly shift the image to avoid tiled boundaries.\n",
    "    shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n",
    "    shift_down, shift_right = shift[0],shift[1] \n",
    "    img_rolled = tf.roll(tf.roll(img, shift_right, axis=1), shift_down, axis=0)\n",
    "    return shift_down, shift_right, img_rolled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiledGradients(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=(\n",
    "            tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=[], dtype=tf.int32),)\n",
    "    )\n",
    "    \n",
    "    def __call__(self, img, tile_size=512):\n",
    "        shift_down, shift_right, img_rolled = random_roll(img, tile_size)\n",
    "\n",
    "    # Initialize the image gradients to zero.\n",
    "        gradients = tf.zeros_like(img_rolled)\n",
    "    \n",
    "    # Skip the last tile, unless there's only one tile.\n",
    "        xs = tf.range(0, img_rolled.shape[0], tile_size)[:-1]\n",
    "        if not tf.cast(len(xs), bool):\n",
    "            xs = tf.constant([0])\n",
    "        ys = tf.range(0, img_rolled.shape[1], tile_size)[:-1]\n",
    "        if not tf.cast(len(ys), bool):\n",
    "            ys = tf.constant([0])\n",
    "\n",
    "        for x in xs:\n",
    "            for y in ys:\n",
    "            # Calculate the gradients for this tile.\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # This needs gradients relative to `img_rolled`.\n",
    "                    # `GradientTape` only watches `tf.Variable`s by default.\n",
    "                    tape.watch(img_rolled)\n",
    "\n",
    "                    # Extract a tile out of the image.\n",
    "                    img_tile = img_rolled[x:x+tile_size, y:y+tile_size]\n",
    "                    loss = calc_loss(img_tile, self.model)\n",
    "\n",
    "            # Update the image gradients for this tile.\n",
    "                gradients = gradients + tape.gradient(loss, img_rolled)\n",
    "\n",
    "        # Undo the random shift applied to the image and its gradients.\n",
    "        gradients = tf.roll(tf.roll(gradients, -shift_right, axis=1), -shift_down, axis=0)\n",
    "\n",
    "        # Normalize the gradients.\n",
    "        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "\n",
    "        return gradients \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_dream_with_octaves(img, steps_per_octave=100, step_size=0.01, octaves=range(-3, -2), octave_scale=1.2):\n",
    "    \n",
    "    base_shape = tf.shape(img)\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "    initial_shape = img.shape[:-1]\n",
    "    img = tf.image.resize(img, initial_shape)\n",
    "    for octave in octaves:\n",
    "        # Scale the image based on the octave\n",
    "        new_size = tf.cast(tf.convert_to_tensor(base_shape[:-1]), tf.float32)*(octave_scale**octave)\n",
    "        img = tf.image.resize(img, tf.cast(new_size, tf.int32))\n",
    "\n",
    "        for step in range(steps_per_octave):\n",
    "            gradients = get_tiled_gradients(img)\n",
    "            img = img + gradients*step_size\n",
    "            img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                show(deprocess(img))\n",
    "                print (\"Octave {}, Step {}\".format(octave, step))\n",
    "    \n",
    "    result = deprocess(img)\n",
    "    base_shape = tf.shape(result)[:-1]\n",
    "\n",
    "    resize_img = tf.image.resize(result, base_shape)\n",
    "    dtype_img = tf.image.convert_image_dtype(resize_img/255.0, dtype=tf.uint8)\n",
    "    # show(result)\n",
    "    \n",
    "    np_image = np.array(dtype_img)\n",
    "    final_image = PIL.Image.fromarray(np_image)\n",
    "\n",
    "    return final_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to translate values from one number range to a new range\n",
    "def translate(value, leftMin, leftMax, rightMin, rightMax):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return round(rightMin + (valueScaled * rightSpan))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['mixed0', 'mixed1', 'mixed2', 'mixed3', 'mixed4', 'mixed5', 'mixed6', 'mixed7', 'mixed8', 'mixed9', 'mixed10']\n",
    "layers = [base_model.get_layer(name).output for name in names]\n",
    "             \n",
    "layer_number = 0\n",
    "image_number = 0\n",
    "min = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickle...\n"
     ]
    }
   ],
   "source": [
    "pickle_exists = os.path.isfile('save_state.pickle')\n",
    "\n",
    "if pickle_exists == True:\n",
    "    \n",
    "    print(\"loading pickle...\")\n",
    "    with open(\"save_state.pickle\", 'rb') as pickle_file:\n",
    "        layer_number = pickle.load(pickle_file)\n",
    "        min = pickle.load(pickle_file) \n",
    "        \n",
    "else:\n",
    "    \n",
    "    print(\"creating new pickle...\")\n",
    "    with open(\"save_state.pickle\", \"wb\") as pickle_file:\n",
    "        pickle.dump(layer_number, pickle_file)\n",
    "        pickle.dump(min, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61a2d5a84bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'frame'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy_images' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(min, len(numpy_images)):\n",
    "    \n",
    "        min = i\n",
    "        print('frame' + str(min) + '.jpg')\n",
    "    \n",
    "        # map current index number to a range between 8, 0 for our convolutional layer\n",
    "        layer_number = translate(min, 0, len(numpy_images) - 1, 5, 6)\n",
    "        print(\"layer_number\", layer_number)\n",
    "\n",
    "        # load models and select layers \n",
    "        dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers[layer_number])\n",
    "        deepdream = DeepDream(dream_model)\n",
    "        get_tiled_gradients = TiledGradients(dream_model)\n",
    "\n",
    "        # save dream image to dream_data directory\n",
    "        dream_img = run_deep_dream_with_octaves(img=numpy_images[min], step_size=0.01)\n",
    "        dream_img.save('./dream_data/dream_frame_' + str(i) + '.png')    \n",
    "    \n",
    "        with open(\"save_state.pickle\", \"wb\") as pickle_file:\n",
    "            pickle.dump(layer_number, pickle_file)\n",
    "            pickle.dump(min, pickle_file)\n",
    "            \n",
    "  \n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
